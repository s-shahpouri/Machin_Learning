{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- are required to program the complete technique of *gradient descent* by hand.\n",
    "- Download and load the file `housing-data.txt`.\n",
    "\n",
    "1. Load the data and create a scatter plot. As you will see, there seems to be a nice linear relationship between the size (on the horizontal axis) and the price (on the vertical axis).\n",
    "- As we have discussed, the general formula for the cost of a model is as follows:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \n",
    "$$\n",
    "\n",
    "2. Write a function `compute_cost` that receives a matrix `X` (of size $m \\times n$), a vector `y` (of size $n \\times 1$) and a vector `theta` (of size $n+1 \\times 1$) and returns the total cost based on the formula above. For this to work correctly, you will need to add a column of 1's to the original `X`-matrix\n",
    "\n",
    "- When we call `compute-cost` with a value of `theta` of `[0,0]`, the total cost will probably be extremely high. In the next step, you need to update the values of this vector in order to minimize $J(\\theta)$. \n",
    "\n",
    "- As we have discussed, the technique we are using for this is *gradient descent*: every step of this descent, we update the values of `theta` as follows:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
    "$$\n",
    "\n",
    "3. Make a function `gradient_descent`, that receives the matrix `X`, the vectors `y` and `theta`, the learning curve `alpha` and a `num_iters`. In this method, performs `num_iters` steps of the gradient descent, calculating the cost $J(\\theta)$ every step and storing that in a list. After the `num_iters`, this function needs to return the found value of `theta` and the list of all the costs.\n",
    "\n",
    "4. Create a plot of the values of $J(\\theta)$ that `compute_costs` has found. Do you see a decrease in the total costs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration from the YAML file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get the dataset path from the configuration\n",
    "dataset = config[\"house_dataset\"][\"path\"]\n",
    "# Load data using np.genfromtxt\n",
    "data = np.genfromtxt(dataset, delimiter=',', dtype=None, names=True, encoding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['size'], df['price'], s=10, c='b', marker='o', alpha=0.5)\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price vs Size of houses')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"compute_cost\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Calculate the cost function for linear regression using the second approach.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Input feature matrix of shape (m, n+1), where m is the number of training examples\n",
    "                           and n is the number of features (including the intercept term).\n",
    "        y (numpy.ndarray): Target values of shape (m,).\n",
    "        theta (numpy.ndarray): Parameter vector of shape (n+1,).\n",
    "\n",
    "    Returns:\n",
    "        float: The computed cost.\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    square_err = (predictions - y) ** 2\n",
    "    return 1 / (2 * m) * np.sum(square_err)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's calculate the cost with theta as [0, 0]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ones'] = 1\n",
    "X = df[['ones', 'size']].values\n",
    "y = df['price'].values\n",
    "theta = np.array([0.0, 0.0])\n",
    "cost = compute_cost(X, y, theta)\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the gradient_descent function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Input feature matrix of shape (m, n+1), where m is the number of training examples\n",
    "                           and n is the number of features (including the intercept term).\n",
    "        y (numpy.ndarray): Target values of shape (m,).\n",
    "        theta (numpy.ndarray): Initial parameter vector of shape (n+1,).\n",
    "        alpha (float): Learning rate.\n",
    "        num_iters (int): Number of iterations to perform gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        list: Cost values and parameter values.\n",
    "\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        predictions = X.dot(theta)\n",
    "        error = np.dot(X.transpose(), (predictions - y))\n",
    "        descent = alpha * 1/m * error\n",
    "        theta -= descent\n",
    "        J_history.append(compute_cost(X, y, theta))\n",
    "\n",
    "    return theta, J_history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a plot of the values of $J(\\theta)$ that compute_costs has found.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of J(θ) values should ideally show a downward trend, steadily decreasing and eventually flattening out when the algorithm has converged to the optimal theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "alpha = 0.01\n",
    "num_iters = 100\n",
    "\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
    "\n",
    "plt.plot(range(1, num_iters + 1), J_history, color='blue')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Cost function using Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But we're not seeing this.\n",
    "The cost is increasing.\n",
    "it is suggested the learning rate is too high and the gradient descent algorithm is overshooting the minimum.\n",
    "\n",
    "When the learning rate is high, the algorithm takes larger steps down the cost function and might not only miss the minimum but also end up at a point where the cost is higher, leading to divergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(J_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the cost function values (J_history) contain inf and nan values. There are some reasons:\n",
    "\n",
    "1- The learning rate:\n",
    "earning rate that's too high. This can cause the gradient descent algorithm to take too large a step, causing numerical instability and resulting in nan values. \n",
    "\n",
    "\n",
    "2- Data Scaling: Gradient Descent is sensitive to the scale of the features.\n",
    "So i seems so important to normalize the data first and then try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size'] = (df['size'] - df['size'].mean()) / df['size'].std()\n",
    "df['price'] = (df['price'] - df['price'].mean()) / df['price'].std()\n",
    "df['ones'] = 1\n",
    "X = df[['ones', 'size']].values\n",
    "y = df['price'].values\n",
    "theta = np.array([0.0, 0.0])\n",
    "cost = compute_cost(X, y, theta)\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01 # decrease learning rate\n",
    "num_iters = 2000 \n",
    "\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
    "\n",
    "plt.plot(range(1, num_iters + 1), J_history, color='blue')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Cost function using Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can see  J(θ), decreases with each iteration. This decrease represents the algorithm getting \"closer\" to the optimal parameters for my linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "Fatemeh and I collaborated on this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
