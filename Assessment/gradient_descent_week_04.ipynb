{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- are required to program the complete technique of *gradient descent* by hand.\n",
    "- Download and load the file `housing-data.txt`.\n",
    "\n",
    "1. Load the data and create a scatter plot. As you will see, there seems to be a nice linear relationship between the size (on the horizontal axis) and the price (on the vertical axis).\n",
    "- As we have discussed, the general formula for the cost of a model is as follows:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \n",
    "$$\n",
    "\n",
    "2. Write a function `compute_cost` that receives a matrix `X` (of size $m \\times n$), a vector `y` (of size $n \\times 1$) and a vector `theta` (of size $n+1 \\times 1$) and returns the total cost based on the formula above. For this to work correctly, you will need to add a column of 1's to the original `X`-matrix\n",
    "\n",
    "- When we call `compute-cost` with a value of `theta` of `[0,0]`, the total cost will probably be extremely high. In the next step, you need to update the values of this vector in order to minimize $J(\\theta)$. \n",
    "\n",
    "- As we have discussed, the technique we are using for this is *gradient descent*: every step of this descent, we update the values of `theta` as follows:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
    "$$\n",
    "\n",
    "3. Make a function `gradient_descent`, that receives the matrix `X`, the vectors `y` and `theta`, the learning curve `alpha` and a `num_iters`. In this method, performs `num_iters` steps of the gradient descent, calculating the cost $J(\\theta)$ every step and storing that in a list. After the `num_iters`, this function needs to return the found value of `theta` and the list of all the costs.\n",
    "\n",
    "4. Create a plot of the values of $J(\\theta)$ that `compute_costs` has found. Do you see a decrease in the total costs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open('housing-data.txt', 'r') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "# Extract column names from the first row\n",
    "column_names = content[0].strip().split(',')\n",
    "\n",
    "# Create a list of dictionaries for each row (excluding the first row)\n",
    "data = []\n",
    "for line in content[1:]:\n",
    "\n",
    "    # Split the line into columns\n",
    "    columns = line.strip().split(',')\n",
    "\n",
    "    # Create a dictionary for the row\n",
    "    row = {}\n",
    "    for i, col_name in enumerate(column_names): \n",
    "\n",
    "        # Convert column value to numeric type\n",
    "        row[col_name] = pd.to_numeric(columns[i])\n",
    "        \n",
    "    # Add the row to the data list\n",
    "    data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['size'], df['price'], s=10, c='b', marker='o', alpha=0.5)\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price vs Size of houses')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"compute_cost\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    square_err = (predictions - y) ** 2\n",
    "    return 1 / (2 * m) * np.sum(square_err)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's calculate the cost with theta as [0, 0]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ones'] = 1\n",
    "X = df[['ones', 'size']].values\n",
    "y = df['price'].values\n",
    "theta = np.array([0.0, 0.0])\n",
    "cost = compute_cost(X, y, theta)\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the gradient_descent function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        predictions = X.dot(theta)\n",
    "        error = np.dot(X.transpose(), (predictions - y))\n",
    "        descent = alpha * 1/m * error\n",
    "        theta -= descent\n",
    "        J_history.append(compute_cost(X, y, theta))\n",
    "\n",
    "    return theta, J_history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a plot of the values of $J(\\theta)$ that compute_costs has found.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of J(θ) values should ideally show a downward trend, steadily decreasing and eventually flattening out when the algorithm has converged to the optimal theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
    "\n",
    "plt.plot(range(1, num_iters + 1), J_history, color='blue')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Cost function using Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But we're not seeing this.\n",
    "The cost is increasing.\n",
    "it is suggested the learning rate is too high and the gradient descent algorithm is overshooting the minimum.\n",
    "\n",
    "When the learning rate is high, the algorithm takes larger steps down the cost function and might not only miss the minimum but also end up at a point where the cost is higher, leading to divergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(J_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the cost function values (J_history) contain inf and nan values. There are some reasons:\n",
    "\n",
    "1- The learning rate:\n",
    "earning rate that's too high. This can cause the gradient descent algorithm to take too large a step, causing numerical instability and resulting in nan values. \n",
    "\n",
    "\n",
    "2- Data Scaling: Gradient Descent is sensitive to the scale of the features.\n",
    "So i seems so important to normalize the data first and then try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size'] = (df['size'] - df['size'].mean()) / df['size'].std()\n",
    "df['price'] = (df['price'] - df['price'].mean()) / df['price'].std()\n",
    "df['ones'] = 1\n",
    "X = df[['ones', 'size']].values\n",
    "y = df['price'].values\n",
    "theta = np.array([0.0, 0.0])\n",
    "cost = compute_cost(X, y, theta)\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01 # decrease learning rate\n",
    "num_iters = 2000 \n",
    "\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
    "\n",
    "plt.plot(range(1, num_iters + 1), J_history, color='blue')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.title('Cost function using Gradient Descent')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can see  J(θ), decreases with each iteration. This decrease represents the algorithm getting \"closer\" to the optimal parameters for my linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "Fatemeh and I collaborated on this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
