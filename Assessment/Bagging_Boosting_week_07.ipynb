{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Understanding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration from the YAML file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get the dataset path from the configuration\n",
    "dataset = config[\"dataset\"][\"path\"]\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset)\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(data.describe())\n",
    "print(\"Missing values:\", data.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis']=data['diagnosis'].map({'M': 1, 'B': 0}).values\n",
    "# Visualization of the target variable distribution\n",
    "sns.countplot(data['diagnosis'])\n",
    "plt.show()\n",
    "print(data['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop(['id','diagnosis'], axis=1)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X.corr()\n",
    "corr_matrix.abs().idxmax()\n",
    "# Strip out the diagonal values for the next step\n",
    "for x in range(len(X.columns)):\n",
    "    corr_matrix.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_matrix# features are highly correlated\n",
    "\n",
    "sns.heatmap(corr_matrix,cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Check the skewness of the features \n",
    "Xscaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "Xscaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the skewness of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .skew 0: no skew, + right skew, - left skew, look for above .75 \n",
    "skew_columns = Xscaled.skew().sort_values(ascending=False)\n",
    "skew_columns = skew_columns.loc[skew_columns > 0.75]\n",
    "skew_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Perform Power Transformation on the skewed columns\n",
    "power_transformer = PowerTransformer()\n",
    "\n",
    "transformed_data = power_transformer.fit_transform(Xscaled[skew_columns])\n",
    "Xscaled[skew_columns] = transformed_data\n",
    "Xscaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xscaled, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging classifier involves training multiple models in parallel with replacement and averaging their predictions, is useful for reducing variance.\n",
    "It can help improve the stability and generalization of the models.\n",
    "\n",
    "Boosting methods, are known for their ability to reduce bias and improve the overall performance of the models.\n",
    "\n",
    "A Dummy Classifier is a type of classifier which does not generate any insight about the data and classifies the given data using only simple rules.\n",
    "This classifier is useful as a simple baseline to compare with other (real) classifiers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = BaggingClassifier(estimator=RandomForestClassifier(), n_estimators=10, random_state=0)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation on the BaggingClassifier\n",
    "rf_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "# Evaluate the BaggingClassifier\n",
    "rf_predictions = clf.predict(X_test)\n",
    "rf_report = classification_report(y_test, rf_predictions)\n",
    "\n",
    "# Define and train an AdaBoost classifier\n",
    "ada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation on the AdaBoost classifier\n",
    "ada_scores = cross_val_score(ada_classifier, X_train, y_train, cv=5)\n",
    "\n",
    "# Evaluate the AdaBoost classifier\n",
    "ada_predictions = ada_classifier.predict(X_test)\n",
    "ada_report = classification_report(y_test, ada_predictions)\n",
    "\n",
    "# Print the cross-validation scores and evaluation reports\n",
    "print(\"BaggingClassifier Cross-Validation Scores:\")\n",
    "print(rf_scores)\n",
    "print(\"BaggingClassifier Evaluation Report:\")\n",
    "print(rf_report)\n",
    "print(\"\\nAdaBoost Cross-Validation Scores:\")\n",
    "print(ada_scores)\n",
    "print(\"AdaBoost Evaluation Report:\")\n",
    "print(ada_report)\n",
    "\n",
    "# Experiment with different configurations for bagging and boosting models\n",
    "# Example: BaggingClassifier with different number of estimators\n",
    "rf_scores = []\n",
    "num_estimators = [50, 100, 200]\n",
    "\n",
    "for n_estimators in num_estimators:\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=5)\n",
    "    rf_scores.append(scores.mean())\n",
    "\n",
    "# Print the cross-validation scores for different number of estimators\n",
    "print(\"\\nBaggingClassifier Cross-Validation Scores for Different Number of Estimators:\")\n",
    "for n, score in zip(num_estimators, rf_scores):\n",
    "    print(f\"Number of Estimators: {n} | Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging classifier with different estimators\n",
    "estimators = [SVC(), RandomForestClassifier(),GradientBoostingClassifier()]\n",
    " \n",
    "for estimator in estimators:\n",
    "    clf = BaggingClassifier(base_estimator=estimator, n_estimators=10, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    report = classification_report(y_test, pred) \n",
    "    print(f\"Bagging with {estimator.__class__.__name__} accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report:\\n{report}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bagging classifiers with different base estimators (SVC, RandomForestClassifier, GradientBoostingClassifier):\n",
    "\n",
    "**Bagging with SVC:**\n",
    "Accuracy: 98%\n",
    "Precision and recall are high for both classes (0 and 1).\n",
    "F1-score indicates balanced performance.\n",
    "\n",
    "**Bagging with RandomForestClassifier:**\n",
    "Accuracy: 94%\n",
    "Precision and recall are slightly lower compared to SVC.\n",
    "F1-score is still good but slightly lower than SVC.\n",
    "\n",
    "**Bagging with GradientBoostingClassifier:**\n",
    "Accuracy: 97%\n",
    "Precision and recall are high for both classes.\n",
    "F1-score indicates balanced performance.\n",
    "\n",
    "In general, all Bagging classifiers perform well and show relatively high accuracy and balanced precision and recall scores for both classes. The choice of base estimator (SVC, RandomForest, or Gradient Boosting) affects the performance slightly, but overall, Bagging effectively improves model accuracy and robustness by combining multiple base models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# List of different numbers of estimators\n",
    "estimator_values = [1, 10, 50, 100, 200]\n",
    "\n",
    "for n_estimators in estimator_values:\n",
    "    # Create a parameter grid for the GridSearchCV\n",
    "    param_grid = {'n_estimators': [n_estimators]}\n",
    "    \n",
    "    # Create a boosting classifier with current n_estimators\n",
    "    clf = GridSearchCV(AdaBoostClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict using the trained boosting classifier\n",
    "    boosting_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    boosting_accuracy = accuracy_score(y_test, boosting_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Boosting with {n_estimators} estimators\")\n",
    "    print(f\"Accuracy: {boosting_accuracy}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, boosting_pred)}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test, boosting_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting with 1 estimators:\n",
    "\n",
    "Accuracy: 0.8947\n",
    "Confusion Matrix: [[63 8] [ 4 39]]\n",
    "In this case, using only 1 estimator results in decent accuracy.\n",
    "\n",
    "Boosting with 10 estimators:\n",
    "Accuracy: 0.9825\n",
    "Confusion Matrix: [[71 0] [ 2 41]]\n",
    "With 10 estimators, the accuracy increases significantly, as seen from both the confusion matrix and classification report. The model is able to predict most of the instances correctly.\n",
    "\n",
    "Boosting with 50 estimators:\n",
    "Accuracy: 0.9737\n",
    "Confusion Matrix: [[70 1] [ 2 41]]\n",
    "Increasing the number of estimators to 50 slightly improves the accuracy, although not significantly. The model performs well with very few misclassifications.\n",
    "\n",
    "Boosting with 100 estimators:\n",
    "\n",
    "Accuracy: 0.9737\n",
    "Confusion Matrix: [[70 1] [ 2 41]]\n",
    "Using 100 estimators, the accuracy remains consistent and the model continues to perform well on the dataset.\n",
    "\n",
    "Boosting with 200 estimators:\n",
    "Accuracy: 0.9737\n",
    "Confusion Matrix: [[70 1] [ 2 41]]\n",
    "Increasing the number of estimators to 200 does not further improve the accuracy, suggesting that the model may have already reached its optimal performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_pred = dummy_clf.predict(X_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_pred)\n",
    "print(f\"Dummy classifier accuracy: {dummy_accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In conclusion, the boosting classifier performed the best followed closely by the bagging classifier with SVC as the base estimator. The RandomForestClassifier-based bagging classifier also performed well but slightly lower than the previous two. The GradientBoostingClassifier-based bagging classifier showed slightly lower performance but still achieved respectable results.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this assignment has been done with help of Fatemeh Rakhshanifar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
