{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11ba2a0",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "In this assignment you first will be introduced to the components of a deep learning model. You will study the code that creates and executes a model and you will apply the theoretical background to propose a model for image data. \n",
    "\n",
    "Learning goals:\n",
    "- understand components of a deep learning model and how they work mathematically\n",
    "- relate the components to the hyperparameters and model setup of keras tensorflow model\n",
    "- propose improvements in design \n",
    "\n",
    "Data:\n",
    "\n",
    "The data we will use is the Breast Cancer Wisconsin (Diagnostic) Data Set of the UCI Machine Learning Repository. You are however free to use your own dataset of interest to study the code. \n",
    "\n",
    "<a name='2'></a>\n",
    "In case you want to study image data classifications you can use a dataset like the Dataset of breast ultrasound imagesfrom Al-Dhabyani W, Gomaa M, Khaled H, Fahmy  2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863\n",
    "\n",
    "\n",
    "Sources used: \n",
    "- https://medium.com/mlearning-ai/binary-classification-of-breast-cancer-diagnosis-using-tensorflow-neural-networks-30ac8f40388\n",
    "- Deep Learning for the Life Sciences by Bharath Ramsundar, Peter Eastman, Pat Walters, Vijay Pande Released April 2019 Publisher(s): O'Reilly Media, Inc. ISBN: 9781492039839 https://www.oreilly.com/library/view/deep-learning-for/9781492039822/\n",
    "\n",
    "\n",
    "\n",
    "# Assignment\n",
    "\n",
    "Study the material and answer the questions\n",
    "\n",
    "1. Study the [background text](#0)\n",
    "2. Study the [code steps](#1). Add comments in your own words and explain design choices such as\n",
    "    - number of [layers](#01), \n",
    "    - [width](#02) of layers, \n",
    "    - number of [epochs](#03), \n",
    "    - [activation functions](#04), \n",
    "    - [loss function](#05), \n",
    "    - [gradient descent function](#06), \n",
    "    - [regularization function](#07)\n",
    "3. Run the [code](#1). Evaluate the performance by discussing the results of the evaluation metrics. What hyper parameters would you recommend to change? Explain your choices. \n",
    "4. How do I set up a `batch_size` and how does it effect the outcome? Why do you think the batch_size was not set in the first place?\n",
    "5. (Optional) Would there be a possibility to execute cross validation? How? \n",
    "6. (Optional) How can I introduce a validation test set? What would I need to change in the code?\n",
    "7. Study the [tensor](#2) text. Consider a dataset of breast cancer images. What needs to be changed to the deep learning model design to make a model based on pictures? You can answer this in words, but if you like you can also try to code the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ab640",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# Study Case\n",
    "\n",
    "Consider the Breast Cancer Wisconsin (Diagnostic) Data Set. UCI Machine Learning Repository: Breast Cancer Wisconsin (diagnostic) data set. (n.d.). https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. Consider the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b01158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration from the YAML file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get the dataset path from the configuration\n",
    "dataset = config[\"dataset\"][\"path\"]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ecde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Preprocess the labels: Convert categorical variable into dummy/indicator variables\n",
    "le = LabelEncoder()\n",
    "le.fit(df['diagnosis'])\n",
    "df['diagnosis'] = le.transform(df['diagnosis'])\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df[df.columns[2:-1]]\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Add first hidden layer with 20 neurons\n",
    "# Add 'relu' activation function\n",
    "model.add(Dense(20, activation='relu'))\n",
    "# Add dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add second hidden layer with 10 neurons and 'relu' activation function\n",
    "model.add(Dense(10, activation='relu'))\n",
    "# Add dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add output layer with 'sigmoid' activation function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary cross entropy loss function and Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "model.fit(x=X_train, y=y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot the loss during training\n",
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions and print the classification report\n",
    "predicted=(model.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n",
    "                                    display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ea39e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ffe15",
   "metadata": {},
   "source": [
    "# 2- Design Choice:\n",
    "### Number and width of layers: \n",
    "The model has 3 layers in total. The first two are hidden layers with 20 and 10 nodes, the final layer is the output layer with one node.\n",
    "Having multiple layers allows the model to learn more complex representations of the data.\n",
    "\n",
    "**Reducing the number of nodes for each subsequent layer is a common practice in deep learning as it forces the model to learn a compressed representation of the input data.**\n",
    "\n",
    "### Number of epochs:\n",
    "The model is trained for <font color='red'>100 epochs.</font>\n",
    "The number of epochs is often chosen based on the complexity of the task, the amount of data available and also the expertise of the data scientict.\n",
    "\n",
    "### Activation functions:\n",
    "The hidden layers use the ReLU (Rectified Linear Unit) activation function. The final layer uses the sigmoid activation function, which is common for binary classification tasks as it outputs probabilities between 0 and 1.\n",
    "\n",
    "### Loss function:\n",
    "The model uses binary cross entropy as its loss function, which is a common choice for binary classification tasks.\n",
    "\n",
    "### Gradient descent function (optimizer):\n",
    "The Adam optimizer is used for the gradient descent. It combines the advantages of two other extensions of stochastic gradient descent: **AdaGrad and RMSProp.**\n",
    "It adapts the learning rate for each weight in the model individually and computes adaptive learning rates for different parameters.\n",
    "\n",
    "### Regularization function:\n",
    "Dropout is used as a regularization technique. Dropout randomly sets a fraction of input units to 0 at each update during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17224d9b",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eec67e",
   "metadata": {},
   "source": [
    "# 3- Performance evaluation and hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f37e8",
   "metadata": {},
   "source": [
    "The model performance is good. The final validation loss is low at 0.0624 and the test set accuracy is quite high at 0.98.\n",
    "\n",
    "This means that model correctly classifies 98% of the test set samples. Moreover, the precision, recall and F1-score are also high (0.98) for both classes, indicating that model has a good balance between precision and recall, and does not favor one class over the other.\n",
    "\n",
    "If we want to experiment with some hyperparameters:\n",
    "\n",
    "- Number of layers and width of layers: Increasing the number of layers and nodes might allow the model to capture more complex patterns in the data. Conversely, reducing them might prevent potential overfitting if it's an issue.\n",
    "\n",
    "- Batch size: You might want to try increasing or decreasing the batch size. A smaller batch size might increase the generalization capability of the model, while a larger batch size could speed up the learning process.\n",
    "\n",
    "- Number of epochs: If the model is not converging, we might need to increase the number of epochs. If the model is overfitting, we might need to decrease the number of epochs or use early stopping.\n",
    "\n",
    "- Dropout rate: If the model is overfitting, we could increase the dropout rate to introduce more regularization. Conversely, if the model is underfitting, we might need to decrease the dropout rate.\n",
    "\n",
    "- Optimizer: Although Adam is used very often and is a solid choice, we can try other optimizers like SGD (with or without momentum) or RMSProp might have a minor impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becd544",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21fe05",
   "metadata": {},
   "source": [
    "# 4- Batch size setup and effect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a3d0f",
   "metadata": {},
   "source": [
    "Batch size is the number of samples processed before the model is updated. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n",
    "\n",
    "The effect of batch_size on the outcome is dependent on several factors. Smaller batch sizes, down to a batch size of 1 (stochastic gradient descent), can result in a model that generalizes better, at the cost of longer training times. Larger batch sizes train faster, but might lead to a model that doesn't generalize as well, and also consumes more memory, because you have to load more data at once.\n",
    "When Batch size is not explicitly set, Keras will use the default batch size of 32. This number is a common choice that tends to work well in many scenarios\n",
    "So we can change the batch_size:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbff57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=100, batch_size=40, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be1b0c",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e072d11",
   "metadata": {},
   "source": [
    "# 5- Cross-validation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74560eb4",
   "metadata": {},
   "source": [
    "Cross-validation can be implemented using the KerasClassifier or KerasRegressor wrapper in scikit-learn. K-fold cross-validation can then be performed using the cross_val_score function from scikit-learn with the wrapped model as input.\n",
    "\n",
    "Introducing a validation test set:\n",
    "A validation set can be introduced during the model.fit call. In the provided code, the test set is used as the validation set. To create a separate validation set, we need to further split the training data into a smaller training set and a validation set. This can be done using train_test_split again on the X_train and y_train.\n",
    "\n",
    "Adapting for image data:\n",
    "For image data, we might want to use convolutional layers (Conv2D for 2D images) in our model as they are designed to automatically and adaptively learn spatial hierarchies of features from the input. We'd also need to reshape our input data so it's suitable for these convolutional layers. The shape should be (batch_size, height, width, channels) for color images, or (batch_size, height, width, 1) for grayscale images.\n",
    "\n",
    "We'll also likely want to include some MaxPooling2D layers after the convolutional layers to downsample the input along its spatial dimensions. Dropout or Batch Normalization might also be helpful to prevent overfitting. Finally, before passing the data to the dense output layer, we'll need to flatten the output from the convolutional and pooling layers.\n",
    "\n",
    "Remember to adjust the loss function and the final layer's activation function if the problem is not binary classification. For multiclass classification, use 'categorical_crossentropy' as loss function and 'softmax' as the activation function in the output layer. For regression, use 'mse' or 'mae' as the loss function and no activation function in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "# Define a function to create the model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=X_scaled.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, verbose=0)\n",
    "\n",
    "# Normalize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# evaluate using 5-fold cross validation\n",
    "results = cross_val_score(model, X_scaled, y, cv=5)\n",
    "print(results.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b86d7",
   "metadata": {},
   "source": [
    "**The mean score of results: 0.9684 returned from the 5-fold cross-validation and it's accuracy of our model.\n",
    "It means that on average, the model correctly classified about 96.84% of the instances across the five different splits of the data into training and testing sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab944538",
   "metadata": {},
   "source": [
    "### Tricky point:\n",
    "input_dim=X_scaled.shape[1]\n",
    "\n",
    "Keras model expects the input shape of the first layer to be provided. we have to specify the input_dim attribute in the first layer of the model preventing error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48d038",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf660f9",
   "metadata": {},
   "source": [
    "# 6- Validation test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e2177",
   "metadata": {},
   "source": [
    "For validation test we can split the data into three sets: training set, validation set, and test set.\n",
    "The training set is used to train the model, the validation set is used to tune parameters and to decide when to stop training (early stopping), and the test set is used to evaluate the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Split the data into training+validation set and test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Then split training+validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Scale the data (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de040d0",
   "metadata": {},
   "source": [
    "Here are my interpretation:\n",
    "Training loss (loss) and validation loss (val_loss) both start at 0.8704 and 0.5503, respectively, indicating that the model isn't fitting the data well initially.\n",
    "\n",
    "As the epochs increase, the loss for both the training and validation sets significantly decreases, which means that the model is learning and improving its performance.\n",
    "\n",
    "The accuracy of the model on the training set (accuracy) starts at 0.4633, but quickly improves to reach 0.9971 by epoch 100, indicating that the model is fitting the training data very well.\n",
    "\n",
    "The validation accuracy (val_accuracy) also increases over time, but it reaches a peak of 0.9649 around epoch 17 and remains roughly constant thereafter, suggesting that the model generalizes well to unseen data.\n",
    "\n",
    "It's also important to note that from around epoch 27 onwards, while the training loss continues to decrease, the validation loss starts to increase. This is an indication of overfitting: the model is starting to memorize the training data and loses its ability to generalize to new data.\n",
    "\n",
    "The final test accuracy is 0.9737, which is close to the final validation accuracy. This suggests that the model is likely to perform similarly on new data in the future.\n",
    "\n",
    "In conclusion, the model seems to be performing quite well, although there are signs of overfitting in the later epochs. We might need to consider strategies to mitigate overfitting, such as early stopping (stopping training when the validation loss starts to increase).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50532435",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36179ae8",
   "metadata": {},
   "source": [
    "# 7- Model based on pictures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2dda66",
   "metadata": {},
   "source": [
    " If the input data are images instead of structured data, the structure of the deep learning model needs to be changed. Instead of Dense layers, we usually use convolutional layers (Conv2D) for image data. Also, we need to ensure that the input shape matches the shape of the images.\n",
    " To adapt the code to work with image data, we would change the data preprocessing steps to handle images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get the dataset path from the configuration\n",
    "image_dataset = config[\"image_dataset\"][\"path\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cceefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set up the ImageDataGenerators like this:\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2) # set validation split\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(image_dataset,\n",
    "                                              target_size=(64, 64),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical',\n",
    "                                              subset='training') # set as training data\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(image_dataset, # same directory as training data\n",
    "                                              target_size=(64, 64),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical',\n",
    "                                              subset='validation') # set as validation data\n",
    "\n",
    "# Set up a simple CNN architecture like this:\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax')) # 3 output neurons for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_set,\n",
    "                    epochs=50,\n",
    "                    validation_data=validation_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1b52e",
   "metadata": {},
   "source": [
    "Both the training loss and the validation loss decrease over time, which is a good sign. This indicates that the model is learning to classify the images correctly.\n",
    "\n",
    "Both the training accuracy and the validation accuracy increase over time, again indicating that the model is learning effectively.\n",
    "\n",
    "The model's performance on the validation set is relatively close to its performance on the training set, which suggests that overfitting is not a significant issue.\n",
    "\n",
    "**Final Performance:**\n",
    "By the 50th epoch, the model achieves a validation accuracy of about 74.92%, which means it correctly classifies approximately 75% of the images in the validation set.\n",
    "\n",
    "In conclusion, the model seems to be performing reasonably well with a final validation accuracy of around 75%. If a higher accuracy is required, experimenting with different model architectures, adding more data, or using data augmentation might help. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28407c5",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65160a",
   "metadata": {},
   "source": [
    "Fatemeh and I did this assignment together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
