{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression & SupportVectorMachines(SVM)\n",
    "\n",
    "Abe both prominent machine learning algorithms utilized predominantly in classification tasks.\n",
    "\n",
    "### Logistic Regression:\n",
    "A probability estimation process that makes use of a logistic or sigmoid function. The sigmoid function consumes a linear combination of the feature set, transforms it, and subsequently outputs a probability value ranging between 0 and 1.\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "is an robust supervised learning algorithm designed for a broad spectrum of tasks, including both classification and regression.\n",
    "\n",
    "SVMs encompass a series of supervised learning techniques that can be applied for classification, regression, and outlier detection. This discussion, however, is specifically centered around classification.\n",
    "SVM effectively processes both linearly separable and non-linearly separable datasets, achieved by leveraging a range of kernel functions that project the data into higher-dimensional feature spaces.\n",
    "The kernel functions most frequently utilized in SVM include linear, polynomial, radial basis function (RBF), and sigmoid. These functions induce non-linearity in the SVM model, empowering it to detect complex data patterns.\n",
    "\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "Highly effective in scenarios with high-dimensional spaces.\n",
    "Still maintains effectiveness even when the quantity of features exceeds the number of samples.\n",
    "Optimized for memory efficiency.\n",
    "Versatility in kernel types, with the emphasis in this discussion being on linear kernels.\n",
    "Limitations:\n",
    "\n",
    "When the quantity of features surpasses the quantity of samples, the application of regularization becomes vital to prevent overfitting.\n",
    "Unlike logistic regression, SVMs don't directly provide probability estimates.\n",
    "Different SVM variants are available in Scikit-learn for classification:\n",
    "\n",
    "### SVC:\n",
    "This variant is based on the libsvm implementation. Its fit time scales quadratically with the dataset size.\n",
    "\n",
    "### NuSVC:\n",
    "Operates similarly to SVC, but also provides control over the number of support vectors.\n",
    "\n",
    "### LinearSVC:\n",
    "Offers a quick implementation for linear kernel SVMs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moon dataset\n",
    "X, y = make_moons(n_samples=200, noise=0, random_state=2, shuffle=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSVC:\n",
    "    \"\"\"Custom SVC class\"\"\"\n",
    "    def __init__(self, kernel='rbf', C=1.0, gamma='scale', degree=3):\n",
    "        \"\"\"Initialize the class with the given parameters\"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model with the given data\"\"\"\n",
    "        self.model = SVC(kernel=self.kernel, C=self.C, gamma=self.gamma, degree=self.degree)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the given data\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the support vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "# iterate over different kernels\n",
    "for i, kernel in enumerate(kernels):\n",
    "\n",
    "    # we create an instance of SVM and fit out data. We do not scale our\n",
    "    model = svm.SVC(kernel=kernel)\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=100, facecolors='none', edgecolors='k', marker='o')\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
    "    \n",
    "    plt.axis('tight')\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    X1, X2 = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\n",
    "\n",
    "    # evaluate model on a grid\n",
    "    decision_function = model.decision_function(np.c_[X1.ravel(), X2.ravel()])\n",
    "    plt.contour(X1, X2, decision_function.reshape(X1.shape), colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot the decision function\n",
    "    plt.title('SVC with {} kernel'.format(kernel))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another wany of drawing these plot, just for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, kernel):\n",
    "\"\"\"\"Plot the decision boundary of the given model\"\"\"\n",
    "\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    # create a meshgrid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # predict the function value for the whole grid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # plot the contour\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='black')\n",
    "    plt.scatter(\n",
    "    model.model.support_vectors_[:, 0], \n",
    "    model.model.support_vectors_[:, 1], \n",
    "    s=50, facecolors='none', edgecolors='red' )\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title(kernel)\n",
    "    plt.show()\n",
    "\n",
    "for kernel in kernels:\n",
    "\n",
    "    # create an instance of the model\n",
    "    model = CustomSVC(kernel=kernel)\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict the response\n",
    "    plot_decision_boundary(model, X_test, y_test, kernel)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel gives us a linear decision boundary, which might not be effective for the moon-shaped dataset. The poly and rbf kernels provide more complex, non-linear decision boundaries that better fit the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(r'.\\Data\\breast-cancer.csv')\n",
    "\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "y = data['diagnosis']\n",
    "# Map 'M' and 'B' to 1 and 0 respectively\n",
    "y = y.map({'M': 1, 'B': 0})\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the positive skewness values indicate a right-skewed distribution.\n",
    "\n",
    "it means the majority of the data is concentrated on the left side of the distribution, and the tail extends towards the right.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "models = [LogisticRegression(), svm.SVC(kernel='linear'), svm.SVC(kernel='rbf')]\n",
    "model_names = ['Logistic Regression', 'SVM (linear kernel)', 'SVM (RBF kernel)']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Classification Report for {model_names[i]}: \")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"AUC-ROC Score:\")\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time to create some Logistic Regression and Support Vector Classifier (SVC) models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression with default hyperparameters\n",
    "lr_model_1 = LogisticRegression()\n",
    "lr_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Model 2: Logistic Regression with different C value\n",
    "lr_model_2 = LogisticRegression(C=0.1)\n",
    "lr_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Model 3: Logistic Regression with different regularization penalty (L1)\n",
    "lr_model_3 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_model_3.fit(X_train, y_train)\n",
    "\n",
    "# Model 4: SVM with default hyperparameters\n",
    "svm_model_1 = SVC(probability=True)\n",
    "svm_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Model 5: SVM with different C value and linear kernel\n",
    "svm_model_2 = SVC(C=0.1, kernel='linear', probability=True)\n",
    "svm_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Model 6: SVM with different gamma value and RBF kernel\n",
    "svm_model_3 = SVC(gamma=0.1, kernel='rbf', probability=True)\n",
    "svm_model_3.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of models\n",
    "models = [lr_model_1, lr_model_2, lr_model_3, svm_model_1, svm_model_2, svm_model_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred) \n",
    "    print(\"Model:\", model)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy:\n",
    "This metric evaluates the overall correctness of a model's predictions. A higher accuracy value indicates that the model is making correct predictions more frequently.\n",
    "\n",
    "### Precision:\n",
    "Precision is the ratio of true positives to the sum of true positives and false positives. In the case of breast cancer diagnosis, precision reflects the model's ability to accurately identify malignant cases (M). A higher precision value means there are fewer false positives, indicating that the model correctly identifies actual positive cases.\n",
    "\n",
    "### Recall:\n",
    "Recall is the ratio of true positives to the sum of true positives and false negatives. For breast cancer diagnosis, recall measures the model's ability to correctly identify all actual positive cases. A higher recall value signifies fewer false negatives, indicating that the model accurately captures most positive cases.\n",
    "\n",
    "### F1-score:\n",
    "The F1-score provides a balanced measure by considering both precision and recall. A higher F1-score indicates a better balance between precision and recall, signifying that the model performs well in capturing positive cases while minimizing false positives and false negatives.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the case of breast cancer diagnosis, the focus is often on minimizing false negatives (missing actual positive cases). Therefore, we would prioritize models with higher recall values while maintaining a reasonable level of precision. Among the evaluated models, SVC with default parameters (Model: SVC()) achieved the highest recall for the (M) class while maintaining a high overall accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has been done with help of Fathemeh Rakhshani-Far"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
