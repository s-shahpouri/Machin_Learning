{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection techniques are commonly employed in predictive maintenance strategies to identify abnormal patterns or behaviors in machine data that may indicate impending failures.\n",
    "\n",
    "By monitoring various sensor readings, operational parameters, or other relevant data from machines, anomaly detection algorithms can learn the normal behavior or expected patterns of the machines during their normal operation. When a deviation from the normal behavior is detected, it may indicate a potential failure or malfunction. The hypothesis is that the sensor readings of a pump will generate not normal values in case of an (upcoming) failure, and these can be determined with anomaly detection. Several algorithms will be used to evaluate the hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the end result plot to evaluate the algorithm's performance. Look for anomalies identified by the algorithm and compare them to known anomalies or instances of abnormal behavior in the data. Assess whether the algorithm successfully captures these anomalies and if it shows promising results in detecting abnormal patterns. Based on the plot analysis, provide argumentation for the validity of the anomaly detection algorithm hypothesis (see above). Discuss how the algorithm effectively captures anomalies in the time series data and why it is a suitable approach for the use case. Support your argument with references to relevant literature that discuss the effectiveness of the chosen algorithm or similar algorithms in detecting anomalies in time series data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm works by isolating anomalies in the data by constructing isolation trees.\n",
    "\n",
    "These trees are binary partitioning trees that randomly select features and split the data along those features until each data point is isolated in its own leaf node.\n",
    "\n",
    "Anomalies are expected to require fewer splits to be isolated, making them stand out from the normal data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key advantages of the Isolation Forest algorithm is its ability to handle high-dimensional datasets and outliers without requiring complex pre-processing or assumptions about the underlying distribution of the data.\n",
    "\n",
    "This makes it particularly suitable for time series data, where anomalies can occur in various forms and patterns. \n",
    "\n",
    "Also the efficiency of the Isolation Forest algorithm is another advantage for anomaly detection in time series data. \n",
    "\n",
    "The algorithm has a linear time complexity, meaning it can process large datasets efficiently. This efficiency is beneficial for analyzing time series data, which often involves processing large volumes of data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several studies have demonstrated the effectiveness of the Isolation Forest algorithm and similar algorithms in detecting anomalies in time series data. For instance, in the survey \"Anomaly Detection in Univariate Time-Series:\n",
    "\n",
    "A Survey on the State-of-the-Art\" by Mohammad Braei and Dr.-Ing. Sebastian Wagner (2020), the authors evaluated the performance of several unsupervised anomaly detection algorithms on univariate time series data.\n",
    "\n",
    "They found that the Isolation Forest algorithm achieves a \"very high AUC-value\", indicating its ability to effectively discriminate between normal and anomalous instances.\n",
    "\n",
    "Reference:\n",
    "Braei, M., & Wagner, S. (2020). Anomaly Detection in Univariate Time-Series: A Survey on the State-of-the-Art. arXiv preprint arXiv:2011.01958. Retrieved from https://arxiv.org/abs/2011.01958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done together with Fateme Rakhshanifar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment focused on Credit Card Fraud Detection using Anomaly Detection techniques, various predictive models were employed to accurately identify normal transactions from fraudulent ones. The dataset used for this task was sourced from Kaggle. The primary goal of fraud detection is to prevent customers from being wrongly charged for fraudulent transactions. Different techniques were explored:\n",
    "\n",
    "### Outlier Detection:\n",
    "Outlier detection approaches consider fraud transactions as points distributed in a multi-dimensional space, aiming to detect statistically significant deviations from regular transactions. Unsupervised anomaly models are used to map data points onto a one-dimensional line using algorithms, generating anomaly scores and labels.\n",
    "\n",
    "### Isolation Forest Algorithm:\n",
    "The Isolation Forest Algorithm is a technique used to detect anomalies by isolating points. It works by randomly selecting features and splitting on values between the maximum and minimum values of the feature to isolate points. Anomalies are easier to isolate due to their distinct behavior, resulting in a shorter path length to separate them from normal observations. This algorithm has low computational complexity and memory usage.\n",
    "\n",
    "### Local Outlier Factor Algorithm:\n",
    "The Local Outlier Factor (LOF) algorithm is an unsupervised method that computes the local density deviation of a data point with respect to its neighbors. It identifies points with substantially lower density compared to their neighbors as outliers. The choice of the number of neighbors affects the sensitivity of this algorithm.\n",
    "\n",
    "### One Class SVM:\n",
    "One-Class SVM is an unsupervised algorithm designed for outlier detection. It learns from normal transactions to create a model representing the data. When introduced to observations far away from the normal behavior, it labels them as outliers with a negative score. Observations close to the normal behavior receive positive scores.\n",
    "\n",
    "### Autoencoders:\n",
    "Autoencoders, a type of neural network used for unsupervised learning, were explored. They learn patterns in the data by creating hidden layers that extract essential information from the input. Autoencoders use non-linear techniques to capture complex patterns in the data, similar to Principal Component Analysis (PCA) but with enhanced capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection (Anomaly Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, silhouette_score, adjusted_rand_score\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats.mstats import winsorize\n",
    "warnings.filterwarnings('ignore')\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration from the YAML file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get the dataset path from the configuration\n",
    "dataset = config[\"anomaly_data\"][\"path\"]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(dataset)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix,cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[df['Class']==0]\n",
    "fraud = df[df['Class']==1]\n",
    "print(f\"normal shape:\", normal.shape)\n",
    "print(f\"fraud shape:\", fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "features = df.columns[:-1]\n",
    "\n",
    "plt.figure(figsize=(20,50))\n",
    "i=1\n",
    "for feature in features:\n",
    "    plt.subplot(11,3,i)\n",
    "    sns.kdeplot(normal[feature], shade= True, label='Normal',color ='pink')\n",
    "    sns.kdeplot(fraud[feature], shade= True, label='Fraud',color ='blue')\n",
    "    i=i+1\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is obvious that only a few of features has correlation with each other and we can neglect rest of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also fraud is  dificault to detect because they are hiding in a lower dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale between (0,1)\n",
    "df_norm = MinMaxScaler().fit_transform(df)\n",
    "df_norm = pd.DataFrame(df_norm, columns=df.columns)\n",
    "df_norm['Class'] = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the transaction class \"0 = NORMAL  & 1 = FRAUD\"\n",
    "normal = df_norm[df_norm['Class']==0]\n",
    "fraud = df_norm[df_norm['Class']==1]\n",
    "print(f\"normal shape:\", normal.shape)\n",
    "print(f\"fraud shape:\", fraud.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select some of the features that make sense and also for reducing the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_value = corr_matrix.iloc[i, j]\n",
    "        feature1 = corr_matrix.columns[i]\n",
    "        feature2 = corr_matrix.columns[j]\n",
    "        \n",
    "        if abs(corr_value) > 0.3:\n",
    "            col_list.extend([feature1, feature2])\n",
    "\n",
    "# Remove duplicates from the list and convert to a comma-separated string\n",
    "col_list = set(col_list)\n",
    "\n",
    "# Remove 'Class' and 'Time' from col_list\n",
    "col_list.remove('Class')\n",
    "col_list.remove('Time')\n",
    "col_list.remove('Amount')\n",
    "print(\"List of columns with correlation > 0.3 or < -0.3:\")\n",
    "print(col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_norm[list(col_list)]\n",
    "y = df_norm['Class']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "outliers_fraction = 1 - (len(normal)/(len(df))) \n",
    "\n",
    "anomaly_algorithms = [\n",
    "    (\"Isolation Forest\",IsolationForest(contamination=outliers_fraction, n_jobs = -1)),\n",
    "    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction)),\n",
    "    (\"Local Outlier Factor\",LocalOutlierFactor(contamination=outliers_fraction, n_jobs = -1)),\n",
    "    (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction))]\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# ## fit the models: Mind you this takes a lot of time!!!!!!!!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "for name, algorithm in anomaly_algorithms:\n",
    "    print(algorithm)\n",
    "\n",
    "    if name == \"Local Outlier Factor\":\n",
    "        y_pred = algorithm.fit_predict(X)\n",
    "    else:\n",
    "        y_pred = algorithm.fit(X).predict(X)\n",
    "    \n",
    "    df[f'{name}'] = y_pred\n",
    "    print('-'*100)\n",
    "    print(f'number of fraud detected')\n",
    "    print(df[f'{name}'].value_counts())\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "outliers_fraction = 1 - (len(normal) / len(df))\n",
    "\n",
    "anomaly_algorithms = [\n",
    "    (\"Isolation Forest\", IsolationForest(contamination=outliers_fraction, n_jobs=-1)),\n",
    "    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction)),\n",
    "    (\"Local Outlier Factor\", LocalOutlierFactor(contamination=outliers_fraction, n_jobs=-1)),\n",
    "    (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n",
    "]\n",
    "\n",
    "\n",
    "for name, clf in anomaly_algorithms:\n",
    "    print(algorithm)\n",
    "\n",
    "    if name == \"Local Outlier Factor\":\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        scores_prediction = clf.negative_outlier_factor_\n",
    "    elif name == 'One-Class SVM':\n",
    "        clf.fit(X)\n",
    "        y_pred = clf.predict(X)\n",
    "    else:\n",
    "        clf.fit(X)\n",
    "        scores_prediction = clf.decision_function(X)\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "    y_pred[y_pred == 1] = 0\n",
    "    y_pred[y_pred == -1] = 1\n",
    "    n_errors = (y_pred != y).sum()\n",
    "    df[f'{name}'] = y_pred\n",
    "    \n",
    "    print(\"{} number of errors: {}\".format(name, n_errors))\n",
    "    print(df[f'{name}'].value_counts())\n",
    "    print(pd.crosstab(y, y_pred))\n",
    "    print('silhouette coefficient:', round(metrics.silhouette_score(df, y_pred, metric='euclidean'), 3))\n",
    "    print('Adjusted Rand index   :', round(metrics.adjusted_rand_score(y, y_pred), 3))\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    # Calculate accuracy (percentage of correctly detected anomalies)\n",
    "    accuracy = df[df['Class'] == 1][f'{name}'].value_counts().get(-1, 0) / len(df[df['Class'] == 1])\n",
    "    print(f'Accuracy using {name}: {accuracy:.2%}')\n",
    "    print('-' * 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "filename = 'outcome.csv'\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the evaluation results of different anomaly detection algorithms:\n",
    "\n",
    "Isolation Forest:\n",
    "Number of anomalies detected: 492\n",
    "Accuracy: 22.97%\n",
    "\n",
    "One-Class SVM:\n",
    "Number of anomalies detected: 493\n",
    "Accuracy: 34.15%\n",
    "\n",
    "Local Outlier Factor:\n",
    "Number of anomalies detected: 492\n",
    "Accuracy: 0.00%\n",
    "Robust Covariance:\n",
    "Number of anomalies detected: 492\n",
    "Accuracy: 22.97%\n",
    "\n",
    "\n",
    "\n",
    "The \"One-Class SVM\" algorithm was able to correctly detect anomalies with the highest accuracy compared to other algorithms. But \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
