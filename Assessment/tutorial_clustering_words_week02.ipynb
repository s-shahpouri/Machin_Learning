{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bef54c3",
   "metadata": {},
   "source": [
    "# Clustering text\n",
    "\n",
    "See also https://towardsdatascience.com/nmf-a-visual-explainer-and-python-implementation-7ecdd73491f8\n",
    "\n",
    "\n",
    "This notebook uses a collection of clinical case reports to cluster words by topics using the NMF method. To cluster text we need to preprocess the text first with regular natural language processing cleaning steps such as remove punctuations, stopwords, or other unwanted text. we lower the text and use the lemma to reduce variation of words. This is all done in part A. \n",
    "\n",
    "Next we need to prepare the text in a document term matrix so that NMF can perform the calculations. The Document-Term Matrix (DTM) represents the frequency of words (or terms) in a collection of documents. Each row in the matrix represents a document, and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document. This is done in part B\n",
    "\n",
    "Lastly we run the clustering algorithm and visualize the outcome. This is done in part C\n",
    "\n",
    "\n",
    "## The data \n",
    "A collection of 200 clinical case report documents in plain text format are used. The documents are named using PubMed document IDs, and have been edited to include only clinical case report details. The dataset is called \"MACCROBAT2020\" and is the second release of this dataset, with improvements made to the consistency and format of annotations\n",
    "https://figshare.com/articles/dataset/MACCROBAT2018/9764942\n",
    "\n",
    "\n",
    "## Portfolio assignment\n",
    "\n",
    "You can use this assignment to fill your portfolio.\n",
    "Read, execute and analyse the code in the notebook. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "a) read the article Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization. you can find the article <a href = 'https://pubmed.ncbi.nlm.nih.gov/26011887/'> here</a>. Explain the similarities of this notebook and the article. Explain in your own words what need to be added to this notebook to reproduce the article. There is no need to code the solution.\n",
    "\n",
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?\n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, for instance text you work with in your project. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f823c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b58de96a",
   "metadata": {},
   "source": [
    "# Part A: get and clean the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fa9acc",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadeb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#create empty dataframe\n",
    "df = pd.DataFrame(columns=['docid','text'])\n",
    "\n",
    "# get all files ending with .txt\n",
    "docs = [x for x in glob.glob(r\"C:\\zshahpouri\\Machin_Learning\\Data\\*.txt\")]\n",
    "\n",
    "#fill dataframe\n",
    "for doc in docs:\n",
    "    txt = Path(doc).read_text(encoding='utf-8', errors='ignore')\n",
    "    docid = Path(doc).stem  # Extract the filename without the extension\n",
    "    df.loc[len(df.index)] = [docid, txt]\n",
    "\n",
    "#set index docid       \n",
    "df = df.set_index('docid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8564d7a",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d33dd2",
   "metadata": {},
   "source": [
    "The code below defines a function called clean_text that takes a text parameter and returns a cleaned version of it. The function first converts the text to lowercase using the `lower()` method. It then removes any text enclosed in square brackets using **regular expressions** and the re.sub() method. Next, it removes any **punctuation** from the text using the `string.punctuation` module and `re.escape()` and `re.sub()` methods. It then removes any words that **contain numbers** using `re.sub()` and a regular expression. Finally, it removes any **read errors** (represented by the � character) using `re.sub()`.\n",
    "\n",
    "The code then defines a lambda function called cleaned that takes a single parameter `x` and applies the `clean_text` function to it. This lambda function can be used to clean text data in a Pandas dataframe or any other data structure that can take a lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f04d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, \n",
    "    remove punctuation, remove read errors,\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub('�', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fba7a135",
   "metadata": {},
   "source": [
    "The function 'nouns' cleans a text and extracts only the nouns from it. It first defines a lambda function called 'is_noun' that checks if a given word is a noun, based on its part of speech (POS) tag. The input text is then tokenized using the `word_tokenize` function from the **nltk** module, which breaks down the text into smaller units called tokens. Tokenization is an important pre-processing step in NLP that helps standardize and prepare text data for further analysis.\n",
    "\n",
    "The function creates a `WordNetLemmatizer` object from the nltk module to lemmatize each word, which is the process of converting a word to its base or dictionary form. Lemmatization helps to reduce the dimensionality of text data and improve the accuracy of the analysis, especially in cases where words have different inflected forms but share the same root.\n",
    "\n",
    "The function uses a list comprehension to lemmatize each word in the tokenized text if it is a noun, and stores the resulting list of lemmatized nouns in the 'all_nouns' variable. Finally, the function returns a string containing the joined list of lemmatized nouns.\n",
    "\n",
    "Note that this function requires the 'nltk' module to be imported and assumes that the 'pos_tag' function from the 'nltk' module is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text\n",
    "df[\"text\"] = df[\"text\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(df[\"text\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouns.text[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd934e72",
   "metadata": {},
   "source": [
    "# Part B: The Document-Term Matrix (DTM)\n",
    "\n",
    "To perform analyses we need to create a Document-Term Maxtrix. \n",
    "The Document-Term Matrix (DTM) represents the frequency of words (or terms) in a collection of documents. Each row in the matrix represents a document, and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document.\n",
    "\n",
    "In specific case below, the DTM has been created using only the nouns extracted from the original text, and has been transformed using the TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme. The TF-IDF scheme assigns weights to words based on how often they appear in a document relative to how often they appear in the entire corpus. Words that appear frequently in a document but infrequently in the corpus are given higher weights, as they are considered to be more important for distinguishing that document from others in the corpus. This weighting scheme is commonly used in text mining and information retrieval to identify key terms or topics in a collection of documents.\n",
    "\n",
    "The resulting DTM can be used for various purposes such as topic modeling, clustering, classification, and visualization of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fec90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term matrix with only nouns\n",
    "# Store TF-IDF Vectorizer\n",
    "tv_noun = TfidfVectorizer(stop_words='english', ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d0ad932",
   "metadata": {},
   "source": [
    "# Part C: Run the NMF\n",
    "\n",
    "This code below performs Non-negative Matrix Factorization (NMF) on the Document Term Matrix (DTM) with only nouns. NMF is a dimensionality reduction technique that decomposes a matrix into two matrices: a document-topic matrix (W) and a topic-term matrix (H).\n",
    "\n",
    "The code first creates an NMF model with 5 topics. The model is then fit to the DTM with the fit_transform() method, which returns the document-topic matrix (W).\n",
    "\n",
    "The `display_topics()` function is called to extract the top words from the topic-term matrix (H) using the feature names from the TF-IDF vectorizer. \n",
    "\n",
    "The resulting output will show the top n words for each of the n topics learned by the NMF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "# Learn an NMF model for given Document Term Matrix 'V' \n",
    "# Extract the document-topic matrix 'W'\n",
    "doc_topic = nmf_model.fit_transform(data_dtm_noun)\n",
    "# Extract top words from the topic-term matrix 'H' \n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45b4a809",
   "metadata": {},
   "source": [
    "For inspiration of visualizations see https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d72ad586",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e35a06e0",
   "metadata": {},
   "source": [
    "## Similarities:\n",
    "\n",
    "This script and the article both focus on the process of extracting meaningful information from text data. Specifically, they both involve the application of Nonnegative Matrix Factorization (NMF) for topic modeling and clustering purposes.\n",
    "\n",
    "## Differences:\n",
    "\n",
    "The script is designed to work with a general texts. It just processes the texts by cleaning them, extracting and lemmatizing nouns, and then applying NMF for topic modeling. It does not differentiate between types of nouns or consider different \"views\" of the data.\n",
    "On the other hand, the article outlines a system specifically for clinical documents. It focuses on extracting medication names and symptom names from clinical notes, which can be seen as a form of feature extraction. The extracted features are then used for clustering using NMF and multi-view NMF.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "070dec07",
   "metadata": {},
   "source": [
    "In order to reproduce the approach from the article using the notebook, the following steps should be considered:\n",
    "\n",
    "### Domain-Specific Text Processing:\n",
    "Rather than just extracting and lemmatizing all nouns, the notebook needs to incorporate a mechanism for specifically extracting medication and symptom names. This could involve the use of domain-specific ontologies, regular expressions, or even machine learning-based named entity recognition (NER) techniques.\n",
    "\n",
    "### Multi-view NMF Implementation:\n",
    "Our script currently uses a basic NMF implementation, but the article uses multi-view NMF. You would need to add code to implement or call a multi-view NMF algorithm.\n",
    "\n",
    "### Symptom/Medication-Based Clustering:\n",
    "The script currently does not perform any clustering, while the article specifically focuses on clustering based on the extracted features (medication and symptom names). Thus, clustering methods should be added to the notebook.\n",
    "\n",
    "### Evaluation and Comparison:\n",
    "The article presents a comparison of the performances of NMF and multi-view NMF on clinical documents clustering. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "165a39ef",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "Fatemeh and I collaborated on this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
