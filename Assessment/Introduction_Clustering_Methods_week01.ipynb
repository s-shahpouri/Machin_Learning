{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Common preprocessing steps:\n",
    "### a. Quality control (Inspecting and cleaning the data):\n",
    "This part typically includes reading the file, inspecting various aspects of the data such as its size/shape, presence of NaN values, and understanding the type of data. After inspecting the data, the next step is to clean it, which involves handling missing values, removing duplicates, or addressing any other data quality issues that may be present.\n",
    "\n",
    "### b. Normalizing and Scaling:\n",
    "is a common preprocessing step that aims to adjust data to a standard range or distribution. It is performed to address differences in magnitudes or scales between different features or variables in the data. Normalization is particularly important when comparing or combining variables that have different units or ranges of values. \n",
    "\n",
    "### c. Feature selection:\n",
    "\n",
    "Feature selection involves identifying and selecting a subset of relevant features from the dataset. This step is beneficial when dealing with high-dimensional data, as it helps reduce noise and computational complexity, leading to improved clustering results. By selecting the most informative features, we can focus on the essential aspects of the data, leading to more effective clustering outcomes.\n",
    "\n",
    "### d. Dimensionality reduction:\n",
    "Dimensionality reduction techniques, such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE), are used to reduce the dimensionality of the data while retaining its key structure. This step helps visualize and analyze the data effectively by reducing noise and highlighting underlying patterns.\n",
    "\n",
    "These preprocessing steps should be executed to ensure the data is of high quality, comparable, and suitable for downstream analysis. However, the specific execution may vary depending on the dataset and analysis goals. For example, if the dataset is already preprocessed and normalized, one may skip those steps and proceed directly to dimensionality reduction and clustering analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial_Clustering_Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization methods are used in the cluster methods tutorial\n",
    "The visualization method used is a dendrogram. The selected method is appropriate for the visualization because it provides insights into the relationship and similarity between clusters, showing which clusters are more similar to each other and how they are connected. Hierarchical clustering groups similar data points into clusters based on a measure of similarity or dissimilarity. The dendrogram can handle large datasets by using truncation or collapsing methods to make the visualization more manageable. In the provided code, the `truncate_mode='lastp'` parameter is used to truncate the dendrogram and show only a subset of the most recent clusters (50 clusters in this case). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial_cluster_scanpy_object\n",
    "## Performance/evaluation metrics:\n",
    "\n",
    "ROC-AUC is a commonly used metric for evaluating binary classification models. The ROC-AUC score ranges from 0 to 1, with a higher value indicating better model performance. In this case, the roc_auc_score function from scikit-learn is used to calculate the ROC-AUC score for each iteration of the loop. \n",
    "\n",
    "ROC-AUC has been chosen for this case because the y is binary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fatemeh and I collaborated on this assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the breast cancer dataset\n",
    "data = pd.read_csv(r'../Data/breast-cancer.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the relevant features for clustering analysis\n",
    "selected_features = data[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean']]\n",
    "selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Normalize or scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform\n",
    "selected_features = data[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean']]\n",
    "scaled_data = scaler.fit_transform(selected_features)\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the data\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(scaled_data)\n",
    "pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Perform clustering analysis on the preprocessed data\n",
    "clustering_algorithm = KMeans(n_clusters=2)  # Choose the number of clusters\n",
    "clusters = clustering_algorithm.fit_predict(reduced_data)\n",
    "\n",
    "#  Visualize the clusters\n",
    "plt.scatter(reduced_data[clusters == 0, 0], reduced_data[clusters == 0, 1], c='blue', label='Cluster 1')\n",
    "plt.scatter(reduced_data[clusters == 1, 0], reduced_data[clusters == 1, 1], c='red', label='Cluster 2')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Clustering Results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
