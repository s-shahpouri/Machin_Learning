{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Common preprocessing steps:\n",
    "### a. Quality control (Inspecting and cleaning the data):\n",
    "This part typically includes reading the file, inspecting various aspects of the data such as its size/shape, presence of NaN values, and understanding the type of data. After inspecting the data, the next step is to clean it, which involves handling missing values, removing duplicates, or addressing any other data quality issues that may be present.\n",
    "\n",
    "### b. Normalizing and Scaling:\n",
    "is a common preprocessing step that aims to adjust data to a standard range or distribution. It is performed to address differences in magnitudes or scales between different features or variables in the data. Normalization is particularly important when comparing or combining variables that have different units or ranges of values. \n",
    "\n",
    "\n",
    "\n",
    "### d. Dimensionality reduction:\n",
    "Dimensionality reduction techniques, such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE), are used to reduce the dimensionality of the data while retaining its key structure. This step helps visualize and analyze the data effectively by reducing noise and highlighting underlying patterns.\n",
    "\n",
    "These preprocessing steps should be executed to ensure the data is of high quality, comparable, and suitable for downstream analysis. However, the specific execution may vary depending on the dataset and analysis goals. For example, if the dataset is already preprocessed and normalized, one may skip those steps and proceed directly to dimensionality reduction and clustering analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial_Clustering_Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization methods are used in the cluster methods tutorial\n",
    "The visualization method used is a dendrogram. The selected method is appropriate for the visualization because it provides insights into the relationship and similarity between clusters, showing which clusters are more similar to each other and how they are connected. Hierarchical clustering groups similar data points into clusters based on a measure of similarity or dissimilarity. The dendrogram can handle large datasets by using truncation or collapsing methods to make the visualization more manageable. In the provided code, the `truncate_mode='lastp'` parameter is used to truncate the dendrogram and show only a subset of the most recent clusters (50 clusters in this case). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial_cluster_scanpy_object\n",
    "## Performance/evaluation metrics:\n",
    "\n",
    "ROC-AUC is a commonly used metric for evaluating binary classification models. The ROC-AUC score ranges from 0 to 1, with a higher value indicating better model performance. In this case, the roc_auc_score function from scikit-learn is used to calculate the ROC-AUC score for each iteration of the loop. \n",
    "\n",
    "ROC-AUC has been chosen for this case because the y is binary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
