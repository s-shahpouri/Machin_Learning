{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Trees and Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.inspection import DecisionBoundaryDisplay \n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A moon shaped synthetic dataset with noise\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "# Add decision trees with varying depth\n",
    "for depth in [1, 2, 5, 10]:\n",
    "    classifiers.append(DecisionTreeClassifier(max_depth=depth))\n",
    "\n",
    "# Add Gaussian Naive Bayes with different var smoothing\n",
    "for var_smoothing in [1e-9, 1e-5, 1e-1]:\n",
    "    classifiers.append(GaussianNB(var_smoothing=var_smoothing))\n",
    "\n",
    "# Train and evaluate all models\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'Classifier: {clf.__class__.__name__}, Parameters: {clf.get_params()}')\n",
    "    print(f'Train Accuracy: {clf.score(X_train, y_train)}')\n",
    "    print(f'Test Accuracy: {clf.score(X_test, y_test)}')\n",
    "    print('-----------------------------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision trees, a depth of 1 may lead to underfitting (poor performance) because the model is too simple and can't capture the complexity of the data.\n",
    "\n",
    "A depth of 10, on the other hand, may lead to overfitting (great performance on the training set, but poor on the test set) because the model is too complex and memorizes the training data, rather than learning from it.\n",
    "\n",
    "The optimal depth should balance between these extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "datasets = [make_moons(), make_circles()]\n",
    "\n",
    "# Create classifiers with different parameters\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=1),  # Decision Tree with max depth 1\n",
    "    DecisionTreeClassifier(),  # Default Decision Tree\n",
    "    DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5),\n",
    "    DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=5, min_weight_fraction_leaf=0.1),\n",
    "    DecisionTreeClassifier(min_impurity_decrease=0.2),   \n",
    "    make_pipeline(StandardScaler(), GaussianNB())  # Naive Bayes with StandardScaler preprocessing\n",
    "]\n",
    "\n",
    "\n",
    "classifier_names = [\n",
    "    'D-Tree (Max Depth 1)',\n",
    "    'D-Tree (default)',\n",
    "    'D-Tree (Custom Parameters)',\n",
    "    'D-Tree (Weighted Leaf)',\n",
    "    'D-Tree (Impurity Decrease)',\n",
    "    'GaussianNB',\n",
    "\n",
    "\n",
    "]\n",
    "figure = plt.figure(figsize=(15, 10))\n",
    "i = 1\n",
    "\n",
    "# Iterate over datasets\n",
    "for ds_count, ds in enumerate(datasets):\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "    if ds_count == 0:\n",
    "        ax.set_title('Input data')\n",
    "\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k')\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\")\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    for name, clf in zip(classifier_names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        DecisionBoundaryDisplay.from_estimator(clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5)\n",
    "\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors=\"k\", alpha=0.6)\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "        if ds_count == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(x_max - 0.3, y_min + 0.3, (\"%.2f\" % score).lstrip(\"0\"), size=15, horizontalalignment=\"right\")\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Assumptions: Different classifiers make different assumptions about the data. For example, Naive Bayes assumes that the features are conditionally independent given the class label, while decision trees make hierarchical decisions based on feature thresholds. These assumptions can lead to different model behaviors and performance depending on the dataset.\n",
    "\n",
    "Dataset Characteristics: The characteristics of the dataset, such as the number of features, the number of samples, and the complexity of the underlying patterns, can influence the performance of different classifiers. Some classifiers may perform well on linearly separable datasets, while others may handle complex nonlinear relationships better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The changes in the behavior of different models can be attributed to the parameters you set. For instance:\n",
    "\n",
    "The depth of the Decision Tree affects its complexity and ability to capture intricate patterns.\n",
    "\n",
    "Different Naive Bayes models make different assumptions about the distribution of data.\n",
    "\n",
    "Parameters like min_samples_split, min_samples_leaf, and min_impurity_decrease control the tree's structure and stopping criteria.\n",
    "min_weight_fraction_leaf and min_impurity_decrease allow us to set thresholds for node splitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Tree Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# Load the configuration from the YAML file\n",
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Get the dataset path from the configuration\n",
    "dataset = config[\"dataset\"][\"path\"]\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "y = data['diagnosis']\n",
    "\n",
    "# Perform feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Check the skewness of the features \n",
    "X = pd.DataFrame(X)\n",
    "X.skew() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "understand the most important features in the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modeling\n",
    "dtc = DecisionTreeClassifier(max_depth=2)\n",
    "scores = cross_val_score(dtc, X, y, cv=5)\n",
    "\n",
    "print(f'Cross-validation mean accuracy: {scores.mean()}')\n",
    "\n",
    "# Training\n",
    "dtc.fit(X, y)\n",
    "\n",
    "# Evaluation\n",
    "print(f'Training set accuracy: {dtc.score(X, y)}')\n",
    "\n",
    "# Explanation\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_tree(dtc, filled=True, feature_names=data.columns, class_names=['Malignant', 'Benign'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree classifier\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "data_malignant = data[data['diagnosis'] == 'M']\n",
    "data_benign = data[data['diagnosis'] == 'B']\n",
    "\n",
    "# Create a scatter plot for malignant tumors\n",
    "plt.scatter(data_malignant['radius_mean'], data_malignant['texture_mean'], color='purple', alpha=0.5, label='Malignant')\n",
    "# Create a scatter plot for benign tumors\n",
    "plt.scatter(data_benign['radius_mean'], data_benign['texture_mean'], color='lime', alpha= 0.5, label='Benign')\n",
    "\n",
    "\n",
    "plt.xlabel('Radius')\n",
    "plt.ylabel('Texture')\n",
    "plt.title(' Average Radius vs Texture')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree classifier, the algorithm makes sequential choices based on feature values, dividing the data at each node according to a certain feature threshold. This continues until a termination condition, like reaching the set maximum depth or the minimum sample split limit, is fulfilled.\n",
    "\n",
    "Features closer to the root node, appearing earlier in the tree, usually hold more importance as they contribute to major data splits. This is not a hard rule, but a general trend. By examining the tree structure and the order of feature splits, we can make educated guesses about which features are more crucial to the classification.\n",
    "\n",
    "Moreover, scikit-learn's decision tree implementation offers a feature_importances_ attribute, which can be used to gain quantitative insights into the importance of each feature. This attribute represents the amount each feature decreases the weighted impurity.\n",
    "\n",
    "By examining these metrics and the structure of the decision tree, we can gain a deeper understanding of the main drivers behind the classification of cases as malignant or benign in the context of the breast cancer dataset. This understanding could prove valuable in determining potential markers for cancer or figuring out the major contributing factors in cancer diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tree plot for malignant tumors\n",
    "fig, ax = plt.subplots(figsize=(25, 20))\n",
    "\n",
    "tree.plot_tree(dtc, \n",
    "               feature_names=X.columns, \n",
    "               class_names=['Benign', 'Malignant'], \n",
    "               filled=True, \n",
    "               fontsize=15, \n",
    "               label='root', \n",
    "               ax=ax)\n",
    "\n",
    "# Show the plot\n",
    "plt.title(\"Decision Tree for Breast Cancer Classification\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the Corona pandemic, seven roommates in a student house did a Corona test. The table below show the data of these students: whether they experiences shivers, had a running nose, or had a headache. The test result is also shown.\n",
    "\n",
    "Roommate | shivers | running nose | headache | test result\n",
    "--|--|--|--|--\n",
    "1 | Y | N | No | Negative\n",
    "2 | N | N | Mild | Negative\n",
    "3 | Y | Y | No | Positive\n",
    "4 | N | Y | No | Negative\n",
    "5 | N | N | Heavy | Positive\n",
    "6 | Y | N | No | Negative\n",
    "7 | Y | Y | Mild | Positive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the Roommate column shouldn't be included because it's just an identifier for each student and doesn't carry any meaningful information for predicting the test result.\n",
    "\n",
    "Including such an identifier as a feature might mislead the classifier into thinking that different roommates have inherent predictive value, which is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a pandas DataFrame with the given data\n",
    "data = {\n",
    "    'shivers': ['Y', 'N', 'Y', 'N', 'N', 'Y', 'Y'],\n",
    "    'running nose': ['N', 'N', 'Y', 'Y', 'N', 'N', 'Y'],\n",
    "    'headache': ['No', 'Mild', 'No', 'No', 'Heavy', 'No', 'Mild'],\n",
    "    'test result': ['Negative', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    " # Transform nominal data into binary columns\n",
    "df_encoded = pd.get_dummies(df.drop('test result', axis=1))\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df_encoded.values\n",
    "y = df['test result']\n",
    "\n",
    "# Train the Categorical Naive Bayes classifier\n",
    "clf = CategoricalNB()\n",
    "clf.fit(df_encoded, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test results\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate the predicted probabilities for the 5th observation\n",
    "observation_5 = df_encoded.iloc[4, :]  # Extract the features of the 5th observation\n",
    "\n",
    "# Predict the probabilities for each class\n",
    "proba = clf.predict_proba([observation_5])\n",
    "\n",
    "# Extract the probability for the Negative class (index 0) and Positive class (index 1)\n",
    "probability_negative = proba[0, 0]\n",
    "probability_positive = proba[0, 1]\n",
    "\n",
    "print(\"Probability of Negative class:\", probability_negative)\n",
    "print(\"Probability of Positive class:\", probability_positive)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual result for observation 5 is \"Positive,\" but the classifier predicted it to be \"Negative.\" This is an error made by the classifier, and it happened because even though the \"Negative\" probability was higher, it was still wrong in this case.\n",
    "\n",
    "In simpler terms, the classifier thought that observation 5 was more likely to be \"Negative,\" and it chose that option even though it was actually \"Positive.\" This shows that sometimes a higher probability doesn't necessarily mean the prediction is correct.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
