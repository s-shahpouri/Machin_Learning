{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Trees and Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.inspection import DecisionBoundaryDisplay  \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A moon shaped synthetic dataset with noise\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "# Add decision trees with varying depth\n",
    "for depth in [1, 2, 5, 10]:\n",
    "    classifiers.append(DecisionTreeClassifier(max_depth=depth))\n",
    "\n",
    "# Add Gaussian Naive Bayes with different var smoothing\n",
    "for var_smoothing in [1e-9, 1e-5, 1e-1]:\n",
    "    classifiers.append(GaussianNB(var_smoothing=var_smoothing))\n",
    "\n",
    "# Train and evaluate all models\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'Classifier: {clf.__class__.__name__}, Parameters: {clf.get_params()}')\n",
    "    print(f'Train Accuracy: {clf.score(X_train, y_train)}')\n",
    "    print(f'Test Accuracy: {clf.score(X_test, y_test)}')\n",
    "    print('-----------------------------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision trees, a depth of 1 may lead to underfitting (poor performance) because the model is too simple and can't capture the complexity of the data.\n",
    "\n",
    "A depth of 10, on the other hand, may lead to overfitting (great performance on the training set, but poor on the test set) because the model is too complex and memorizes the training data, rather than learning from it.\n",
    "\n",
    "The optimal depth should balance between these extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [make_moons(), make_circles()]\n",
    "names = ['Decision Tree', 'Naive Bayes']\n",
    "classifiers = [\n",
    "    tree.DecisionTreeClassifier(),  # Original decision tree\n",
    "    tree.DecisionTreeClassifier(max_depth = 1),  # Decision tree with max depth of 1\n",
    "    tree.DecisionTreeClassifier(min_samples_leaf=10),  # Decision tree with minimum samples per leaf set to 10\n",
    "    GaussianNB()  # Naive Bayes classifier\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10, 5))\n",
    "\n",
    "i = 1\n",
    "\n",
    "# iterate over datasets\n",
    "for ds_count, ds in enumerate(datasets):\n",
    "    X, y = ds\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=0\n",
    "    )\n",
    "    \n",
    "    # determining min point and max point and add margin\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    # plot dataset\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    \n",
    "    # plot original\n",
    "    if ds_count == 0:\n",
    "        ax.set_title('Input data')\n",
    "        \n",
    "    # plot training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k')\n",
    "    \n",
    "    # plot testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\")\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "    \n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        \n",
    "        # make pipeline to make it easier\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "        )\n",
    "        \n",
    "        # plot training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # plot testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        \n",
    "        if ds_count == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Assumptions: Different classifiers make different assumptions about the data. For example, Naive Bayes assumes that the features are conditionally independent given the class label, while decision trees make hierarchical decisions based on feature thresholds. These assumptions can lead to different model behaviors and performance depending on the dataset.\n",
    "\n",
    "Dataset Characteristics: The characteristics of the dataset, such as the number of features, the number of samples, and the complexity of the underlying patterns, can influence the performance of different classifiers. Some classifiers may perform well on linearly separable datasets, while others may handle complex nonlinear relationships better.\n",
    "\n",
    "Model Complexity: The complexity of the model, controlled by parameters like the maximum depth of a decision tree or the number of neighbors in a k-nearest neighbors classifier, can affect its performance. A more complex model may have higher capacity and can fit the training data more closely, but it may also be prone to overfitting if the dataset is small or noisy.\n",
    "\n",
    "Feature Scaling and Preprocessing: Different classifiers may have different sensitivity to the scale and distribution of features. Some classifiers, like Naive Bayes, assume that the features follow a Gaussian distribution, while others, like decision trees, are less sensitive to feature scaling. Preprocessing steps such as standardization or normalization can influence the performance of different classifiers.\n",
    "\n",
    "Hyperparameter Tuning: The performance of a classifier can be influenced by the choice of hyperparameters. Hyperparameters control the behavior of the model and are not learned from the data. Grid search or other optimization techniques can be used to find the optimal hyperparameter values, which can significantly impact the model's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Tree Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(r'../Data/breast-cancer.csv')\n",
    "\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "y = data['diagnosis']\n",
    "\n",
    "# Perform feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Check the skewness of the features \n",
    "X = pd.DataFrame(X)\n",
    "X.skew() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "understand the most important features in the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modeling\n",
    "dtc = DecisionTreeClassifier(max_depth=2)\n",
    "scores = cross_val_score(dtc, X, y, cv=5)\n",
    "\n",
    "print(f'Cross-validation mean accuracy: {scores.mean()}')\n",
    "\n",
    "# Training\n",
    "dtc.fit(X, y)\n",
    "\n",
    "# Evaluation\n",
    "print(f'Training set accuracy: {dtc.score(X, y)}')\n",
    "\n",
    "# Explanation\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_tree(dtc, filled=True, feature_names=data.columns, class_names=['Malignant', 'Benign'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree classifier, the algorithm makes sequential choices based on feature values, dividing the data at each node according to a certain feature threshold. This continues until a termination condition, like reaching the set maximum depth or the minimum sample split limit, is fulfilled.\n",
    "\n",
    "Features closer to the root node, appearing earlier in the tree, usually hold more importance as they contribute to major data splits. This is not a hard rule, but a general trend. By examining the tree structure and the order of feature splits, we can make educated guesses about which features are more crucial to the classification.\n",
    "\n",
    "Moreover, scikit-learn's decision tree implementation offers a feature_importances_ attribute, which can be used to gain quantitative insights into the importance of each feature. This attribute represents the amount each feature decreases the weighted impurity.\n",
    "\n",
    "By examining these metrics and the structure of the decision tree, we can gain a deeper understanding of the main drivers behind the classification of cases as malignant or benign in the context of the breast cancer dataset. This understanding could prove valuable in determining potential markers for cancer or figuring out the major contributing factors in cancer diagnosis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the Roommate column shouldn't be included because it's just an identifier for each student and doesn't carry any meaningful information for predicting the test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a pandas DataFrame with the given data\n",
    "data = {\n",
    "    'shivers': ['Y', 'N', 'Y', 'N', 'N', 'Y', 'Y'],\n",
    "    'running nose': ['N', 'N', 'Y', 'Y', 'N', 'N', 'Y'],\n",
    "    'headache': ['No', 'Mild', 'No', 'No', 'Heavy', 'No', 'Mild'],\n",
    "    'test result': ['Negative', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "df_encoded = pd.get_dummies(df.drop('test result', axis=1))\n",
    "\n",
    "# Extract the target variable\n",
    "y = df['test result']\n",
    "\n",
    "# Train the Categorical Naive Bayes classifier\n",
    "clf = CategoricalNB()\n",
    "clf.fit(df_encoded, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate the predicted probabilities for the 5th observation\n",
    "observation_5 = df_encoded.iloc[4, :]  # Extract the features of the 5th observation\n",
    "\n",
    "# Predict the probabilities for each class\n",
    "proba = clf.predict_proba([observation_5])\n",
    "\n",
    "# Extract the probability for the Negative class (index 0) and Positive class (index 1)\n",
    "probability_negative = proba[0, 0]\n",
    "probability_positive = proba[0, 1]\n",
    "\n",
    "print(\"Probability of Negative class:\", probability_negative)\n",
    "print(\"Probability of Positive class:\", probability_positive)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually calculate the prediction for observation 5, we use Bayes' theorem. Note that we need to calculate the probabilities for each feature given both classes (Positive and Negative) and then calculate the total probability for both classes. After that, we normalize these probabilities so they sum to 1. The class with the highest probability is the predicted class. This is the method used by Naive Bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
